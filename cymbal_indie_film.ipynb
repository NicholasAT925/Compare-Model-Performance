{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Task 1 Set Up"
      ],
      "metadata": {
        "id": "S0RtEpl_zi9w"
      },
      "id": "S0RtEpl_zi9w"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet google-genai nest-asyncio==1.5.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEluWfBjykq4",
        "outputId": "6d7791ec-c083-40c3-ac44-2da5f340658e"
      },
      "id": "xEluWfBjykq4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/200.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.0/200.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from vertexai.evaluation import (\n",
        "    MetricPromptTemplateExamples,\n",
        "    EvalTask,\n",
        "    PairwiseMetric,\n",
        "    PairwiseMetricPromptTemplate,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        ")\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "Nz7NAQJFy91j"
      },
      "id": "Nz7NAQJFy91j",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-01-6cd835eecace\"\n",
        "LOCATION = \"us-central1\"\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "6PQd-yTszPq2"
      },
      "id": "6PQd-yTszPq2",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2. Explore example data and generate a document"
      ],
      "metadata": {
        "id": "Wl3F20Pczg3I"
      },
      "id": "Wl3F20Pczg3I"
    },
    {
      "cell_type": "code",
      "source": [
        "hourly_rates = cleandoc(\"\"\"\n",
        "  Screenwriter: $40\n",
        "  Actor: $25\n",
        "  Director: $30\n",
        "  Camera Operator: $35\n",
        "  Sound Engineer: $20\n",
        "  Editor: $30\n",
        "  \"\"\")\n",
        "\n",
        "planning_notes = cleandoc(\"\"\"\n",
        " Phases of Production:\n",
        "   Writing:\n",
        "   The Screenwriter will write the script.\n",
        "   They need 72 hours to do so.\n",
        "\n",
        "\n",
        "   Pre-Production:\n",
        "   The Director needs time to analyze the script.\n",
        "   They will work on it for 36 hours.\n",
        "   The Camera Operator will join the director for 24 hours of planning.\n",
        "\n",
        "\n",
        "   Production Phase 1\n",
        "   The first three days of filming will require the director, 4 actors, the camera operator, and the sound engineer\n",
        "\n",
        "\n",
        "   Production Phase 2\n",
        "   The next three days of filming will require the director, 8 actors, the camera operator, and the sound engineer\n",
        "\n",
        "\n",
        "   Post-Production\n",
        "   The editor will take 64 hours to edit the film.\n",
        "   The director will work with the editor for 24 hours during this phase.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Xgnol1u_zacQ"
      },
      "id": "Xgnol1u_zacQ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = [\n",
        "    \"\"\"What is the cost of each phase of production?\n",
        "    If days are mentioned, assume an 8 hour work day.\"\"\",\n",
        "\n",
        "    \"\"\"How many days will each phase require? Assume an\n",
        "    8 hour work day. If multiple people are working in parallel,\n",
        "    do not add those times together, but only use the longest time.\n",
        "    Also include a count of the total number of days of the entire\n",
        "    project.\"\"\",\n",
        "\n",
        "    \"\"\"Prepare a text schedule for all phases of the film starting\n",
        "    on Feb 3, 2025. The whole crew should be off Saturdays\n",
        "    and Sundays.\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "aK9nbNVpzq4t"
      },
      "id": "aK9nbNVpzq4t",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = cleandoc(\"\"\"\n",
        "  <instructions>\n",
        "  Prepare a document to fulfill the task based on the context provided.\n",
        "  </instructions>\n",
        "<task>\n",
        "  {task}\n",
        "  </task>\n",
        "<context>\n",
        "  {context}\n",
        "  </context>\n",
        "  \"\"\")"
      ],
      "metadata": {
        "id": "mMQs6ryezslv"
      },
      "id": "mMQs6ryezslv",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Model"
      ],
      "metadata": {
        "id": "Z-vqTn0Nz2Ds"
      },
      "id": "Z-vqTn0Nz2Ds"
    },
    {
      "cell_type": "code",
      "source": [
        "llm_pro = GenerativeModel(\n",
        "  \"gemini-2.5-pro-preview-05-06\",\n",
        "  generation_config={\n",
        "      \"temperature\": 0,\n",
        "  },\n",
        ")\n",
        "\n",
        "llm_flash = GenerativeModel(\n",
        "  \"gemini-2.0-flash-001\",\n",
        "  generation_config={\n",
        "      \"temperature\": 0,\n",
        "  },\n",
        ")"
      ],
      "metadata": {
        "id": "H4jw5zswz1n6"
      },
      "id": "H4jw5zswz1n6",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine hourly_rates and planning_notes (with a pair of line breaks as a separator) to form a context chunk."
      ],
      "metadata": {
        "id": "GzaxvfcO0H1_"
      },
      "id": "GzaxvfcO0H1_"
    },
    {
      "cell_type": "code",
      "source": [
        "context = hourly_rates + \"\\n\\n\" + planning_notes"
      ],
      "metadata": {
        "id": "AxgaXUWk0LJL"
      },
      "id": "AxgaXUWk0LJL",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the prompt template and the context, generate a response to the second task (tasks[1]) for each model (llm_pro and llm_flash)."
      ],
      "metadata": {
        "id": "J2OPIHAe0_h0"
      },
      "id": "J2OPIHAe0_h0"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.format(task=tasks[1], context=context)\n",
        "\n",
        "response_pro = llm_pro.generate_content(prompt)\n",
        "response_flash = llm_flash.generate_content(prompt)"
      ],
      "metadata": {
        "id": "i-i7Z6kQ0vRc"
      },
      "id": "i-i7Z6kQ0vRc",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Markdown() class imported from IPython.display to wrap the response text to render Gemini's responses, which are often formatted as Markdown strings."
      ],
      "metadata": {
        "id": "DsqnmhdS1hE3"
      },
      "id": "DsqnmhdS1hE3"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(\"# Gemini Pro Response\\n\\n\" + response_pro.text))\n",
        "display(Markdown(\"# Gemini Flash Response\\n\\n\" + response_flash.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fwszBNpu1hzD",
        "outputId": "e61b55b5-2baa-497f-afbd-6e10a5557ce6"
      },
      "id": "fwszBNpu1hzD",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Gemini Pro Response\n\nOkay, let's break down the project timeline phase by phase, assuming an 8-hour workday.\n\n**Phase Durations:**\n\n1.  **Writing:**\n    *   Screenwriter: 72 hours\n    *   Days: 72 hours / 8 hours/day = **9 days**\n\n2.  **Pre-Production:**\n    *   Director: 36 hours\n    *   Camera Operator (with Director): 24 hours\n    *   Since these activities happen in parallel (or the Camera Operator's time is within the Director's), we take the longest duration.\n    *   Longest duration: 36 hours\n    *   Days: 36 hours / 8 hours/day = **4.5 days**\n\n3.  **Production Phase 1:**\n    *   The context explicitly states: \"The first three days of filming...\"\n    *   Days: **3 days**\n\n4.  **Production Phase 2:**\n    *   The context explicitly states: \"The next three days of filming...\"\n    *   Days: **3 days**\n\n5.  **Post-Production:**\n    *   Editor: 64 hours\n    *   Director (with Editor): 24 hours\n    *   Since these activities happen in parallel (the Director works *with* the Editor), we take the longest duration.\n    *   Longest duration: 64 hours\n    *   Days: 64 hours / 8 hours/day = **8 days**\n\n**Summary of Days per Phase:**\n\n*   **Writing:** 9 days\n*   **Pre-Production:** 4.5 days\n*   **Production Phase 1:** 3 days\n*   **Production Phase 2:** 3 days\n*   **Post-Production:** 8 days\n\n**Total Number of Days for the Entire Project:**\n\nTotal Days = 9 (Writing) + 4.5 (Pre-Production) + 3 (Production 1) + 3 (Production 2) + 8 (Post-Production)\nTotal Days = **27.5 days**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Gemini Flash Response\n\n**Project Timeline Breakdown**\n\nHere's a breakdown of the project timeline, assuming an 8-hour workday:\n\n**Phase Breakdown:**\n\n*   **Writing:**\n    *   Screenwriter: 72 hours\n    *   Days Required: 72 hours / 8 hours/day = 9 days\n\n*   **Pre-Production:**\n    *   Director: 36 hours\n    *   Camera Operator: 24 hours\n    *   Since the director and camera operator are working in parallel, we take the longest time.\n    *   Days Required: 36 hours / 8 hours/day = 4.5 days\n\n*   **Production Phase 1:**\n    *   Director, 4 Actors, Camera Operator, Sound Engineer: 3 days\n    *   Days Required: 3 days\n\n*   **Production Phase 2:**\n    *   Director, 8 Actors, Camera Operator, Sound Engineer: 3 days\n    *   Days Required: 3 days\n\n*   **Post-Production:**\n    *   Editor: 64 hours\n    *   Director: 24 hours\n    *   Since the editor and director are working in parallel, we take the longest time.\n    *   Days Required: 64 hours / 8 hours/day = 8 days\n\n**Total Project Days:**\n\n9 days (Writing) + 4.5 days (Pre-Production) + 3 days (Production Phase 1) + 3 days (Production Phase 2) + 8 days (Post-Production) = **27.5 days**\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3. Prepare the Evaluation Dataset and EvalTask"
      ],
      "metadata": {
        "id": "mwDyc_WD134H"
      },
      "id": "mwDyc_WD134H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Evaluation DataFrame"
      ],
      "metadata": {
        "id": "Y6BiyhVO2uoo"
      },
      "id": "Y6BiyhVO2uoo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare evaluation data\n",
        "eval_dataset = pd.DataFrame({\n",
        "    \"input\": [prompt],  # the formatted prompt from earlier\n",
        "    \"baseline_output\": [response_pro.text],  # Response A (Pro)\n",
        "    \"output\": [response_flash.text],         # Response B (Flash)\n",
        "})"
      ],
      "metadata": {
        "id": "0foq30n51l1t"
      },
      "id": "0foq30n51l1t",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Pairwise Metric & EvalTask"
      ],
      "metadata": {
        "id": "-j2PtMcv3ETW"
      },
      "id": "-j2PtMcv3ETW"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_task = EvalTask(\n",
        "    dataset=eval_dataset,\n",
        "    metrics=[\n",
        "        MetricPromptTemplateExamples.Pairwise.QUESTION_ANSWERING_QUALITY\n",
        "    ],\n",
        "    experiment=\"indie-film-planning\"\n",
        ")"
      ],
      "metadata": {
        "id": "0eSwZRrd3FYW"
      },
      "id": "0eSwZRrd3FYW",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MetricPromptTemplateExamples.get_prompt_template('groundedness'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlSmUawp6pMw",
        "outputId": "e3468299-304a-4030-e464-fc92a2c66a82"
      },
      "id": "RlSmUawp6pMw",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# Instruction\n",
            "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\n",
            "We will provide you with the user input and an AI-generated response.\n",
            "You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the criteria provided in the Evaluation section below.\n",
            "You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step by step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
            "\n",
            "\n",
            "# Evaluation\n",
            "## Metric Definition\n",
            "You will be assessing groundedness, which measures the ability to provide or reference information included only in the user prompt.\n",
            "\n",
            "## Criteria\n",
            "Groundedness: The response contains information included only in the user prompt. The response does not reference any outside information.\n",
            "\n",
            "## Rating Rubric\n",
            "1: (Fully grounded). All aspects of the response are attributable to the context.\n",
            "0: (Not fully grounded). The entire response or a portion of the response is not attributable to the context provided by the user prompt.\n",
            "\n",
            "## Evaluation Steps\n",
            "STEP 1: Assess the response in aspects of Groundedness. Identify any information in the response not present in the prompt and provide assessment according to the criterion.\n",
            "STEP 2: Score based on the rating rubric. Give a brief rationale to explain your evaluation considering Groundedness.\n",
            "\n",
            "\n",
            "# User Inputs and AI-generated Response\n",
            "## User Inputs\n",
            "### Prompt\n",
            "{prompt}\n",
            "\n",
            "## AI-generated Response\n",
            "{response}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.evaluation import run_eval_task"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "Fz5j_M0Z8XxG",
        "outputId": "3ae98251-2822-498d-f8bd-7c84abdb8199"
      },
      "id": "Fz5j_M0Z8XxG",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'run_eval_task' from 'vertexai.evaluation' (/usr/local/lib/python3.11/dist-packages/vertexai/evaluation/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-bac8db02188e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_eval_task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'run_eval_task' from 'vertexai.evaluation' (/usr/local/lib/python3.11/dist-packages/vertexai/evaluation/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_response = eval_task()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "eOX_Tb3M7NH3",
        "outputId": "b1b6d09a-99a1-4ac4-e048-7b0417673a1a"
      },
      "id": "eOX_Tb3M7NH3",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'EvalTask' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2bb04069cbc9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'EvalTask' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_task.summary_table\n",
        "eval_task.metrics_table\n",
        "eval_task.metrics_table[\"preferred_response\"]\n",
        "eval_task.metrics_table[\"explanation\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "2CZDCY2s8m-x",
        "outputId": "23ad53d3-5d53-4c27-8a4b-f68fcdda4bf8"
      },
      "id": "2CZDCY2s8m-x",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'EvalTask' object has no attribute 'summary_table'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-a91d39d80299>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meval_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preferred_response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meval_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"explanation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'EvalTask' object has no attribute 'summary_table'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.evaluation.evaluation import EvaluationRunner\n",
        "\n",
        "runner = EvaluationRunner()\n",
        "eval_response = runner.evaluate(eval_task)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "JpCIKAb09IOs",
        "outputId": "e3e6c690-d048-418a-919a-ad02c8575700"
      },
      "id": "JpCIKAb09IOs",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vertexai.evaluation.evaluation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4a1384cd7a9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationRunner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluationRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meval_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vertexai.evaluation.evaluation'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "cymbal-indie-film"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}